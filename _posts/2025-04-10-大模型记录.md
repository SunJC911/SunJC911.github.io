---
title: 大模型记录.
date: 2025-04-10 20:48:00 +0800
categories: [LLM]
tags: [LLM, Ollama]

author: [sjc]

description: 跟上时代潮流，自己来动手.
render_with_liquid: true  # for video

mermaid: true  # for mermaid


---



干瞪眼五分钟，愣一个字没动，下意识在想怎么写标题写大纲，怎么写才能最完美。

干！写起来先！

## 想做这个的原因

实际工作中更偏向于写业务的算法，但是想着自己怎么着也叫“算法工程师”，却对大模型几乎一窍不通，这怎么行？

于是乎想着部署一个LLM，动手学习一下，补充一下知识。

看着看着，想到公司需要在入职培训的时候投入大量的时间和精力，因为连锁超市的门店员工普遍干不了多久。

所以我做这个有几个目的：

1. 动手实践一下部署LLM，边动手边学。而不是一直先想太多，导致没动力去迈出第一步。
2. 往入职培训助手那去靠拢，万一公司需要呢。不要也行，就当练手。
3. 对现在的主流，有一个认识。如果以后有要结合的地方，可以更快上手。

## 设备

1. 远程服务器：Linux，x86_64
2. 代理服务器：Linux
3. 本地：Mac Mini M4 24G

远程服务器在某内部，无法直接从外部访问，使用了一个 代理服务器来做端口穿透。通过这种方式，外部请求可以通过代理服务器转发到远程服务器。

# Ollama

## Ollama部署

Ollama官网：[click here](https://ollama.com/)

### 远程服务器

#### 安装

官网的`curl -fsSL https://ollama.com/install.sh | sh`太慢了。

[click here](https://github.com/ollama/ollama/releases) 这里下载对应版本的 tgz，建议使用迅雷下载。

我有百度云会员，故用 [bypy](https://github.com/houtianze/bypy)传至远程服务器。

接着按照[官方 linux 步骤](https://github.com/ollama/ollama/blob/main/docs/linux.md)进行配置，默认端口 11434。

Ollama 常用的环境变量`ollama help serve`(**！需要学习！**)

#### 安全性

启动服务后，通过`sudo lsof -i -P -n | grep 11434`得知 Ollama 只在本地接口（127.0.0.1）上监听端口 11434，没有把端口暴露给公网。

若后续需要将端口暴露到公网，则**！另外学习！**。

#### 建立隧道

`ssh -L 11434:localhost:11434 username@111.111.111.111 -p 1111`

输入密码建立成功后，在新 cmd 中输入`curl http://localhost:11434`得到`Ollama is running`表示成功。

#### Quickstart

`ollama run llama3.2`，等待模型参数下载后，即可在远程服务器进行对话。

### 本地

#### 调用

确保远程已经 download了想使用的模型，以及建立了 ssh 隧道。

本地输入

````bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "你好吗？请你只输出一个字。",
  "stream": false
}'
````

本地输出 Ollama 推理 API 的标准响应

````json
{
	"model": "llama3.2",
	"created_at": "2025-04-11T01:50:39.889219666Z",
	"response": "好",
	"done": true,
	"done_reason": "stop",
	"context": [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 57668, 53901, 103054, 39045, 57668, 92780, 67117, 220, 16, 109820, 19113, 128009, 128006, 78191, 128007, 271, 53901],
	"total_duration": 77196406,
	"load_duration": 28357222,
	"prompt_eval_count": 36,
	"prompt_eval_duration": 23391493,
	"eval_count": 2,
	"eval_duration": 24445403
}
````



## open-webUI 部署

open-webUI 官网：[click here](https://github.com/open-webui/open-webui)

### 远程服务器

#### 安装

先注册 HuggingFace ，然后安装 HuggingFace`pip install huggingface-hub`，然后登录`huggingface-cli login`， 在官网创建 token(read access 全部点上)，然后在 cmd 输入 token。

记得先运行 Ollama。

接着安装 open-webui，`pip install open-webui`，然后启动`open-webui serve`

#### 建立隧道

`ssh -L 8080:localhost:8080 username@111.111.111.111 -p 1111`

####  本地

网页输入http://localhost:8080/，自动连接 Ollama。

## 探索 Ollama 部署自定义模型

Ollama 官网导入模型手册（没参考）：[click here](https://github.com/ollama/ollama/blob/main/docs/import.md)

llama.cpp官网 github：[click here](https://github.com/ggml-org/llama.cpp)

### hf下载模型

模型可以在https://huggingface.co/，或者huggingface镜像网站https://hf-mirror.com/，或者魔搭社区进行下载，我用魔搭社区的python脚本进行下载，下载速度直接拉满，执行前需要先运行pip install modelscope 。
`````python
from modelscope import snapshot_download
 
#模型存放路径
model_path = r'D:\huggingface'
#模型名字
name = 'itpossible/Chinese-Mistral-7B-Instruct-v0.1'
model_dir = snapshot_download(name, cache_dir=model_path, revision='master')
`````

### hf模型转为gguf

git clone llama.cpp用于转换，git clone太慢，还是bypy靠谱

安装相关环境`pip install -r llama.cpp/requirements/requirements-convert_hf_to_gguf.txt`

````bash
# 如果不量化，保留模型的效果
python llama.cpp/convert_hf_to_gguf.py ./qwen2_0.5b_instruct  --outtype f16 --verbose --outfile qwen2_0.5b_instruct_f16.gguf
# 如果需要量化（加速并有损效果），直接执行下面脚本就可以
python llama.cpp/convert_hf_to_gguf.py ./qwen2_0.5b_instruct  --outtype q8_0 --verbose --outfile qwen2_0.5b_instruct_q8_0.gguf
````

### 创建Modelfile

确保ollama正在运行。

```yaml
FROM /mnt/workspace/qwen2-0.5b-instruct-q8_0.gguf

# set the temperature to 0.7 [higher is more creative, lower is more coherent]
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER repeat_penalty 1.05
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>"""
# set the system message
SYSTEM """
You are a helpful assistant.
"""
```

### 创建与运行自定义模型

```bash
ollama create qwen2_0.5b_instruct --file ./ModelFile
```

```bash
ollama run qwen2_0.5b_instruct
```

# vLLM



# 微调大模型

# 后续

探索下 vllm

探索量化

探索构建数据集

探索微调

# 参考链接

[ollama+open-webui，本地部署自己的大模型](https://blog.csdn.net/spiderwower/article/details/138463635?spm=1001.2014.3001.5502)

[将 HuggingFace 模型转换为 GGUF 及使用 ollama 运行——以Qwen为例](https://zhuanlan.zhihu.com/p/721617987)
