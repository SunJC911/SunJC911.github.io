---
title: Ollama 部署记录.
date: 2025-04-10 20:48:00 +0800
categories: [LLM]
tags: [LLM, Ollama]

author: [sjc]

description: 跟上时代潮流，自己来动手.
render_with_liquid: true  # for video

mermaid: true  # for mermaid


---



干瞪眼五分钟，愣一个字没动，下意识在想怎么写标题写大纲，怎么写才能最完美。

干！写起来先！

## 想做这个的原因

实际工作中更偏向于写业务的算法，但是想着自己怎么着也叫“算法工程师”，却对大模型几乎一窍不通，这怎么行？

于是乎想着部署一个LLM，动手学习一下，补充一下知识。

看着看着，想到公司需要在入职培训的时候投入大量的时间和精力，因为连锁超市的门店员工普遍干不了多久。

所以我做这个有几个目的：

1. 动手实践一下部署LLM，边动手边学。而不是一直先想太多，导致没动力去迈出第一步。
2. 往入职培训助手那去靠拢，万一公司需要呢。不要也行，就当练手。
3. 对现在的主流，有一个认识。如果以后有要结合的地方，可以更快上手。

## 设备

1. 远程服务器：Linux，x86_64
2. 代理服务器：Linux
3. 本地：Mac Mini M4 24G

远程服务器在某内部，无法直接从外部访问，使用了一个 代理服务器来做端口穿透。通过这种方式，外部请求可以通过代理服务器转发到远程服务器。

## 部署

Ollama官网：https://ollama.com/

#### 远程服务器

##### 安装

官网的`curl -fsSL https://ollama.com/install.sh | sh`太慢了。

https://github.com/ollama/ollama/releases 这里下载对应版本的 tgz，建议使用迅雷下载。

我有百度云会员，故用 [bypy](https://github.com/houtianze/bypy)传至远程服务器。

接着按照[官方 linux 步骤](https://github.com/ollama/ollama/blob/main/docs/linux.md)进行配置，默认端口 11434。

Ollama 常用的环境变量`ollama help serve`(需要学习)

##### 安全性

启动服务后，通过`sudo lsof -i -P -n | grep 11434`得知 Ollama 只在本地接口（127.0.0.1）上监听端口 11434，没有把端口暴露给公网。

若后续需要将端口暴露到公网，则另外学习。

##### 建立隧道

`ssh -L 11434:localhost:11434 username@111.111.111.111 -p 1111`

输入密码建立成功后，在新 cmd 中输入`curl http://localhost:11434`得到`Ollama is running`表示成功。

##### Quickstart

`ollama run llama3.2`，等待模型参数下载后，即可对话。

**那么如何让本地来调用呢？**
