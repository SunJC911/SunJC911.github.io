---
title: Ollama 部署记录.
date: 2025-04-10 20:48:00 +0800
categories: [LLM]
tags: [LLM, Ollama]

author: [sjc]

description: 跟上时代潮流，自己来动手.
render_with_liquid: true  # for video

mermaid: true  # for mermaid


---



干瞪眼五分钟，愣一个字没动，下意识在想怎么写标题写大纲，怎么写才能最完美。

干！写起来先！

## 想做这个的原因

实际工作中更偏向于写业务的算法，但是想着自己怎么着也叫“算法工程师”，却对大模型几乎一窍不通，这怎么行？

于是乎想着部署一个LLM，动手学习一下，补充一下知识。

看着看着，想到公司需要在入职培训的时候投入大量的时间和精力，因为连锁超市的门店员工普遍干不了多久。

所以我做这个有几个目的：

1. 动手实践一下部署LLM，边动手边学。而不是一直先想太多，导致没动力去迈出第一步。
2. 往入职培训助手那去靠拢，万一公司需要呢。不要也行，就当练手。
3. 对现在的主流，有一个认识。如果以后有要结合的地方，可以更快上手。

## 设备

1. 远程服务器：Linux，x86_64
2. 代理服务器：Linux
3. 本地：Mac Mini M4 24G

远程服务器在某内部，无法直接从外部访问，使用了一个 代理服务器来做端口穿透。通过这种方式，外部请求可以通过代理服务器转发到远程服务器。

## Ollama部署

Ollama官网：[click here](https://ollama.com/)

### 远程服务器

#### 安装

官网的`curl -fsSL https://ollama.com/install.sh | sh`太慢了。

[click here](https://github.com/ollama/ollama/releases) 这里下载对应版本的 tgz，建议使用迅雷下载。

我有百度云会员，故用 [bypy](https://github.com/houtianze/bypy)传至远程服务器。

接着按照[官方 linux 步骤](https://github.com/ollama/ollama/blob/main/docs/linux.md)进行配置，默认端口 11434。

Ollama 常用的环境变量`ollama help serve`(**！需要学习！**)

#### 安全性

启动服务后，通过`sudo lsof -i -P -n | grep 11434`得知 Ollama 只在本地接口（127.0.0.1）上监听端口 11434，没有把端口暴露给公网。

若后续需要将端口暴露到公网，则**！另外学习！**。

#### 建立隧道

`ssh -L 11434:localhost:11434 username@111.111.111.111 -p 1111`

输入密码建立成功后，在新 cmd 中输入`curl http://localhost:11434`得到`Ollama is running`表示成功。

#### Quickstart

`ollama run llama3.2`，等待模型参数下载后，即可在远程服务器进行对话。

### 本地

#### 调用

确保远程已经 download了想使用的模型，以及建立了 ssh 隧道。

本地输入

````bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "你好吗？请你只输出一个字。",
  "stream": false
}'
````

本地输出 Ollama 推理 API 的标准响应

````json
{
	"model": "llama3.2",
	"created_at": "2025-04-11T01:50:39.889219666Z",
	"response": "好",
	"done": true,
	"done_reason": "stop",
	"context": [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 57668, 53901, 103054, 39045, 57668, 92780, 67117, 220, 16, 109820, 19113, 128009, 128006, 78191, 128007, 271, 53901],
	"total_duration": 77196406,
	"load_duration": 28357222,
	"prompt_eval_count": 36,
	"prompt_eval_duration": 23391493,
	"eval_count": 2,
	"eval_duration": 24445403
}
````



## open-webUI 部署

open-webUI 官网：[click here](https://github.com/open-webui/open-webui)

### 远程服务器

#### 安装

先注册 HuggingFace ，然后安装 HuggingFace`pip install huggingface-hub`，然后登录`huggingface-cli login`， 在官网创建 token(read access 全部点上)，然后在 cmd 输入 token。

记得先运行 Ollama。

接着安装 open-webui，`pip install open-webui`，然后启动`open-webui serve`

#### 建立隧道

`ssh -L 8080:localhost:8080 username@111.111.111.111 -p 1111`

####  本地

网页输入http://localhost:8080/，自动连接 Ollama。

## ！探索 Ollama 部署自定义模型！

Ollama 官网导入模型手册：[click here](https://github.com/ollama/ollama/blob/main/docs/import.md)

## 参考链接

[ollama+open-webui，本地部署自己的大模型](https://blog.csdn.net/spiderwower/article/details/138463635?spm=1001.2014.3001.5502)
